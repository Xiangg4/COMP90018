使用广度优先搜索的方式让agent获得了一个检验功能，使agent在处于fake goal与real goal之间的一条最优路径内不会学习另一个agent的reward。

修改了遇到墙的处理方式，变为使agent在撞墙之后会在学习之前重新选择。

加入了一个discount factor。

在可见的未来内，如果墙不在LDP与goal的路径之间这个系统至少不会在show的时候卡死在某个点了……
